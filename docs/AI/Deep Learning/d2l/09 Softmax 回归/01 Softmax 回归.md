# 01 Softmax 回归
Softmax 回归是一个分类问题
![](images/Pasted%20image%2020240905133754.png)

## 回归 vs 分类
- 回归估计一个连续值（房价预测）
- 分类预测一个离散类别（物品分类）

> 连续值是指可以在一定范围内取任意值的变量。
> 离散类别是指变量只能取有限的几个特定值


## 从回归到多类分类
![](images/Pasted%20image%2020240905133938.png)

## 从回归到多类分类 - 均方损失
在分类不关心实际的值，关心的是：是否对正确类别的置信度特别大，将真正的类和不一样的类区分开；希望输出是一个概率

- 对类别进行一位有效编码
- 使用均方损失训练
- 最大值最为预测

### One-Hot Encoding
- **One-Hot Encoding** 是一种将分类变量（例如文本标签）转换成数值形式的技术。
- 在 One-Hot Encoding 中，每个类别都会被表示为一个独立的二进制变量（或位），其中只有一个位是激活状态（通常是1），其余位都是非激活状态（通常是0）。
- 这种编码方式能够将分类数据转换为机器学习算法更容易处理的形式。
- 例如，如果有三个类别 `A`, `B`, `C`，那么标签 `A` 将编码为 `[1, 0, 0]`，标签 `B` 将编码为 `[0, 1, 0]`，标签 `C` 将编码为 `[0, 0, 1]`。


## Softmax 和交叉熵损失
交叉熵常用来衡量两个概率区别
### 1. Softmax 函数
Softmax 函数是一个常用的激活函数，用于将一个向量转换为概率分布。它经常被用于多类分类任务的输出层，使得每个类别的输出可以被视为属于该类的概率。

### 2. 交叉熵损失
交叉熵损失（Cross-Entropy Loss）是一种衡量预测概率分布与真实概率分布之间差异的损失函数。在多类分类任务中，它通常用于评估模型预测的概率分布与实际标签之间的差距。

> 损失函数（Loss Function）是机器学习和深度学习中非常重要的概念之一。它用于量化模型预测与真实标签之间的差异，是模型训练过程中优化目标的核心组成部分。通过最小化损失函数，我们可以调整模型的参数，使其更好地拟合训练数据。