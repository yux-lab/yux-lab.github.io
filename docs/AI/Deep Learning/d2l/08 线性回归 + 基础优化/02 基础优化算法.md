# 02 基础优化算法
## 梯度下降
梯度下降是一种优化算法，用于找到函数的最小值点。
假设你在山上迷路了，想要尽快下山。你站在山顶，四周都是雾，你看不清下山的路，但你可以感觉到地面的坡度（梯度/斜率）。你的目标是尽快到达山脚下，也就是找到最低点。
#### 过程
1. **确定当前位置**：你站在一个较高的地方。
2. **计算梯度**：你观察周围地形，找到最陡的地方，这就是梯度。
3. **选择方向**：沿着最陡的方向走。
4. **选择步长**：确定你每次移动的距离，太大可能错过最低点，太小则寻找速度慢。（学习率）
5. **迭代更新位置**：每走一步后，再次观察地形，继续朝最陡的方向走。
6. **停止条件**：当你发现周围地形平坦时（即梯度接近于零），你就到达了最低点。

## 小批量随机梯度下降
在实际中很少使用梯度下降，深度学习默认的求解算法为小批量随机梯度下降。
随机采样 b 个样本来近似损失

$$
公式
$$
- b 是批量大小，一个重要的超参数；另一个是学习率。

### 超参数与模型参数的区别

- **模型参数（Parameters）**：这些参数是通过训练过程从数据中学习得到的，它们直接决定了模型如何对输入数据进行预测。例如，在线性回归模型中，权重 β 和截距 β0​ 就是模型参数。
- **超参数（Hyperparameters）**：这些参数不是通过训练过程学习得到的，而是由开发者预先设定的，它们影响模型的训练过程和模型本身的架构。超参数的选择通常依赖于经验、实验和调优。

### 常见的超参数
1. **学习率（Learning Rate）**：
    
    - 学习率决定了模型在训练过程中更新参数的速度。较小的学习率可能导致训练过程缓慢，而较大的学习率可能导致训练不稳定或无法收敛。

2. **批次大小（Batch Size）**：
    
    - 批次大小是指在每次更新模型参数时使用的样本数量。较大的批次大小可以提高训练速度，但也可能使模型陷入局部最优解。

3. **迭代次数（Number of Epochs）**：
    
    - 迭代次数是指在整个训练数据集上完整遍历的次数。更多的迭代次数通常意味着更充分的训练，但也可能导致过拟合。

4. **隐藏层大小（Hidden Layer Sizes）**：
    
    - 在神经网络中，隐藏层的大小（神经元数量）决定了模型的复杂度。较大的隐藏层可能提高模型的表达能力，但也可能导致过拟合。

5. **正则化系数（Regularization Strength）**：
    
    - 正则化系数用于控制模型复杂度，避免过拟合。例如，L1或L2正则化可以惩罚较大的权重值，从而简化模型。

6. **优化算法及其参数**：
    
    - 选择不同的优化算法（如梯度下降、随机梯度下降、Adam等）会影响模型的训练过程。优化算法的参数（如动量、学习率衰减等）也需要调整。

7. **丢弃率（Dropout Rate）**：
    
    - 在深度学习中，丢弃率用于控制神经网络中的神经元随机失活的比例，从而减少过拟合。

8. **卷积核大小（Kernel Size）**：
    
    - 在卷积神经网络中，卷积核的大小决定了模型如何捕获局部特征。

9. **池化层大小（Pooling Size）**：
    
    - 池化层用于降低特征图的空间维度，池化层的大小决定了降维的程度。

10. **早停（Early Stopping）**：
    
    - 早停是一种防止过拟合的技术，通过监控验证集上的性能来决定何时停止训练。
