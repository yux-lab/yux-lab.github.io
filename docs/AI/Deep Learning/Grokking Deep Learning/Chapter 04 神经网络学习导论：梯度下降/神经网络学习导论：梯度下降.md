# 4.1 预测、比较和学习

# 4.2 什么是比较
> 为预测的“误差”提供度量

与真实结果相差多少

**均方误差**

# 4.3 学习
> 告诉权重应该如何改变以降低误差

**梯度下降**

# 4.4 比较：你的神经网络是否做出了好的预测？

```python
know_weight = 0.5

input = 0.5

goal_pred = 0.8

pred = input * know_weight

error = (pred - goal_pred) ** 2

print(error)
//0.30250000000000005
```

goal_pred:目标值
为什么误差要平方？使之为正，不会互相抵消

# 4.5 为什么需要测量误差？
> 能够简化问题

> 在测量误差的不同方法中，误差的优先级不同
放大较大的误差，忽略较小的误差

# 4.6 最简单的神经学习形式是什么？
**冷热法学习**
> 通过调整权重来确定哪个方向可以使得误差的降低幅度最大，基于此向该方向移动，重复直到误差趋于0

学习——调整权重——降低误差(使之趋于0)

神经网络学习的本质：搜索。搜到网络的误差降到0

# 4.7 冷热学习
加一点减一点，就像热水器一样

```python
weight = 0.5
input = 0.5
goal_prediction = 0.8

step_amount = 0.001 #对每个迭代，权重应该进行多大幅度调节

for iteration in range(1101): #重复学习，使误差越来越小
    prediction = input *weight
    error = (prediction - goal_prediction) ** 2

    print("Error:" + str(error) + " Prediction:" + str(prediction))

    up_prediction = input * (weight + step_amount) #试试提升权重
    up_error = (goal_prediction - up_prediction) ** 2

    down_prediction = input * (weight - step_amount) #试试降低权重
    down_error = (goal_prediction - down_prediction) ** 2

    #哪个结果更好执行哪个
    if(down_error < up_error):
        weight = weight - step_amount

    if(down_error > up_error):
        weight = weight + step_amount

#Error:0.30250000000000005 Prediction:0.25 Error:0.3019502500000001 
#...
#Error:2.2500000000980924e-06 Prediction:0.7984999999999673 #Error:1.000000000065505e-06 Prediction:0.7989999999999673 #Error:2.5000000003280753e-07 Prediction:0.7994999999999672 #Error:1.0799505792475652e-27 Prediction:0.7999999999999672
```

# 4.7 冷热学习的特点
简单

但
1. 效率低下
- 多次预测才能进行权重更新
2. 难以准确预测目标


# 4.9 基于误差调节权重
梯度下降

```python
weight = 0.5
input = 0.5
goal_pred = 0.8

for iteration in range(20): #重复学习，使误差越来越小
    pred = input *weight
    error = (pred - goal_prediction) ** 2

    #direction_and_amount：我们希望如何更改权重
    #(pred - goal_pred)：纯误差；预测值和真实值的差值
    #缩放、负值反转和停止：将误差转换为我们需要的权重调节的绝对幅度
    direction_and_amount = (pred - goal_pred) * input
    weight = weight - direction_and_amount

    print("Error:" + str(error) + " Prediction:" + str(pred))
# Error:5.408208020258491e-06 Prediction:0.7976744445781151
```

# 4.10 梯度下降的一次迭代
待学....

# 4.19 如何使用导数来学习
![](images/Pasted%20image%2020240616094847.png)

# 4.24 引入α
防止过度修正权重
```python
#4.21
weight = 0.5
goal_pred = 0.8
input = 2
alpha = 0.1

for iteration in range(20): #重复学习，使误差越来越小
    pred = input * weight
    error = (pred - goal_pred) ** 2
    derivative = input * (pred - goal_pred)
    weight = weight - (alpha * derivative)

    print("Error:" + str(error) + " Prediction:" + str(pred))
    
#Error:0.03999999999999998 Prediction:1.0
#Error:0.0144 Prediction:0.92
#Error:4.125769919393652e-10 Prediction:0.8000203119913337
#Error:1.485277170987127e-10 Prediction:0.8000121871948003
```
